neural net base:
	neuron coord:
		[ ] const length array to store which vertex in H_i it corresponds to
		[ ] MAYBE: for added neurons (input, output, etc)

	neuron object:
		[ ] neuron coord
		[ ] charge, and time at which the charge was recieved
		[ ] exists only for neurons with charge or mod neurons, existence of all others is implied but not stored explicitly

	mod edge:
		[ ] exists only for edges which have modified weights or edges that do not exist in the original 
		[ ] stores two neuron coords

	mod edge:
		[ ] stores edges using in-neuron coordinate as key
		[ ] should have function to add an edge, taking two neuron coords and weight as input

	edge weight hash fctn:
		[ ] modified randomly at beginning, then choose highest of a set
		[ ] this is to attempt to minimize the number of edges that are modified
		[ ] should take lots of edges to weight 0

	neuron container:
		[ ] sort by time (smallest first), then by coordinate
		[ ] for timestep t, run through all neuron inputs that are at at time t
		[ ] before pushing along axon, make sure the next neuron in the queue is a different coordinate
			- cheaper to add first push later, instead of push for every input
			- also if non-linear activation function, need to add together all inputs first

	graph object:
		[ ] for graphs that are product components, we assume they are well connected and use an adjacency matrix
		[ ] matricies should be of size specified at compile time
			accomplished by having batch file that takes special text file as input, 
			replaces certain consts, and then recompiles whole neural network

	i/o + new neurons:
		[ ] need to decide whether or not neurons can be added to the network at all
		[ ] need to decide whether input/output neurons are part of the tensor product graph G or outside it

		
learning algorithm:
	[ ] ???
	[ ] basic version using simplified backpropogation, for benchmark


chemical signalling:
	hash function:
		[ ] maps "chemical signature" to set of edges (or maybe neurons)
		[ ] should be easily modifiable


testing:
	[ ] look into using https://github.com/balisujohn/mltests
	[ ] look into openai gym

































neural net proj:
	neurons:
		[ ] have array of "synapse" objects that acts as the axon
		synapse:
			[ ] ptr to destination neuron
			[ ] weight
			[ ] time delay
				this might not be important, figuring out whether this plays a role is one of proj goals
			
		voltage waveform:
			this is only roughly planned out, needs serious optimization, maybe total rethinking
			[ ] vector of voltage values over time
			combining waveforms:
				[ ] function to (weighting) add one waveform (signal from dendrite) to another
				[ ] this checks for going over the threshold voltage
				[ ] if over threshold, add to queue (sorted by time of threshold cross)
	astrocytes:
		[ ] main storage/deletion of neurons
			vec should be fine for storage
		[ ] handle creation of new synapses
			this is something that needs serious thought
		[ ] look into what else they do
		[ ] look into other weird cell types?

	brain:
		[ ] stores astrocytes
		[ ] holds main time loop function
		[ ] queues for threshold crossing should be global for easy access?
			want to be able to create adversary brains though

	learning alg:
		[ ] start with random mutations as baseline
		???????? where do we go from here? (backpropogation is lame)



	report:
		[ ] visualization of network graphs
			python packages exist for this
		[ ] animations of problem solving / playing games





IRL, Astrocyte cells handle creating synapses. 
In my old code, for nonlinearity, big problem is how do I store pointers to all the neurons efficiently, and how do I handle connecting two neurons.
the idea is to create the equivalent of Astrocyte cells, that hold pointers to a lot of neurons and create new connections, as well as maintain metadata.

during every timestep, we iterate over all neurons and send out a "signal" along the axons and add to the voltage of the neurons it points to

